<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="​    人工神经网络（Aritificial Neural Network，ANN）是一系列数学模型。这些模型对人脑的神经元网络进行抽象，构造人工神经元，并按照一定的拓扑结构建立人工神经元之间的连接。人工神经网络又称为神经网络（Neural Network，NN）和神经模型（Neural Model）。 ​    最早的时候，神经网络是一种主要的连接主义模型。连接主义模型还有分布式并行处理模型（">
<meta property="og:type" content="article">
<meta property="og:title" content="《神经网络与深度学习》-前馈神经网络">
<meta property="og:url" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="你电吴彦祖的Blog">
<meta property="og:description" content="​    人工神经网络（Aritificial Neural Network，ANN）是一系列数学模型。这些模型对人脑的神经元网络进行抽象，构造人工神经元，并按照一定的拓扑结构建立人工神经元之间的连接。人工神经网络又称为神经网络（Neural Network，NN）和神经模型（Neural Model）。 ​    最早的时候，神经网络是一种主要的连接主义模型。连接主义模型还有分布式并行处理模型（">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C1.jpg">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C2.jpg">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C3.jpg">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C4.jpg">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C5.jpg">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C6.jpg">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C7.jpg">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C8.jpg">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C9.jpg">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C10.jpg">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C11.jpg">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C12.jpg">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C13.jpg">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C14.jpg">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C15.jpg">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C16.jpg">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C17.jpg">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C18.jpg">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C19.jpg">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C20.jpg">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C21.jpg">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C22.jpg">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C23.jpg">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C24.jpg">
<meta property="og:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C25.jpg">
<meta property="article:published_time" content="2020-06-11T15:24:28.744Z">
<meta property="article:modified_time" content="2020-06-12T09:14:46.876Z">
<meta property="article:author" content="zys">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/.%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C1.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://732632827.github.io/2020/06/11/《神经网络与深度学习》-前馈神经网络/"/>





  <title>《神经网络与深度学习》-前馈神经网络 | 你电吴彦祖的Blog</title>
  








<meta name="generator" content="Hexo 4.2.1"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">你电吴彦祖的Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://732632827.github.io/2020/06/11/%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="zys">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="你电吴彦祖的Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">《神经网络与深度学习》-前馈神经网络</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-06-11T23:24:28+08:00">
                2020-06-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>​    <strong>人工神经网络</strong>（Aritificial Neural Network，ANN）是一系列数学模型。这些模型对人脑的神经元网络进行抽象，构造人工神经元，并按照一定的拓扑结构建立人工神经元之间的连接。人工神经网络又称为<strong>神经网络</strong>（Neural Network，NN）和<strong>神经模型</strong>（Neural Model）。</p>
<p>​    最早的时候，神经网络是一种主要的连接主义模型。连接主义模型还有分布式并行处理模型（Parallel Distributed Processing，PDP），具有3特性：（1）信息表示是分布式的（非局部的）；（2）记忆和知识是存储在单元之间的连接上；（3）通过逐渐改变单元之间的连接强度来学习新的知识。</p>
<p>​    连接主义的神经网络模型具有多种网络结构和<strong>学习方法</strong>。如今流行的主要是采用<strong>误差反向传播</strong>进行学习的神经网络。</p>
<h2 id="1-神经元和激活函数"><a href="#1-神经元和激活函数" class="headerlink" title="1. 神经元和激活函数"></a>1. 神经元和激活函数</h2><p>​    <strong>人工神经元</strong>（Artificial Neuron）或<strong>神经元</strong>（Neuron）是神经网络的<strong>基本单元</strong>。</p>
<p>​    假设一个神经元接收 𝐷 个输入$x_1$, $\cdots$ , $x_D$，令向量$\pmb{𝒙} = [𝑥<em>1 ; 𝑥_2; \cdots ; 𝑥_𝐷]$ 来表示这组输入，用净输入$z \in \mathbb{R}$ 表示一个神经元所获得的输入信号x的加权和，<br>$$<br>\begin{align}<br>z &amp; = \sum</em>{d=1}^D w_d x_d + b \<br> &amp; = \pmb{w}^T\pmb{x} + b \<br> \end{align}<br>$$<br>其中$\pmb{w} = [w_1 ; w_2; \cdots ; w_𝐷]\in \mathbb{R}^D$ 是D维的权重向量， $b \in \mathbb{R}$ 是偏置。</p>
<p>​    净输入 $z$ 在经过经过非线性函数 $f(·)$ 后，得到神经元的<strong>活性值</strong>（Activation）a，<br>$$<br>a = f(z)<br>$$<br>其中非线性函数 $f(·)$ 又称为<strong>激活函数</strong>（Activation Function）。</p>
<p><strong>激活函数</strong> 激活函数很重要，为了增强网络的表示能力和学习能力，激活函数应具有以下性质：</p>
<ul>
<li><p>连续可导（允许少数点不可导）的非线性函数，可导的激活函数可以直接利用数值优化的方法来学习网络参数。</p>
</li>
<li><p>激活函数及其导函数要尽可能的简单，有利于提高网络计算效率。</p>
</li>
<li><p>激活函数的导函数的值域要在一个合适的区间内，不能太大也不能太小，否则会影响训练的效率和稳定性。</p>
<p>​</p>
<p>典型的神经元结构图：</p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C1.jpg" alt="1"></p>
</li>
</ul>
<h3 id="1-1-Sigmoid-型函数"><a href="#1-1-Sigmoid-型函数" class="headerlink" title="1.1  Sigmoid 型函数"></a>1.1  Sigmoid 型函数</h3><p>​    Sigmoid 型函数是指一类S型曲线函数，常用的Sigmoid型函数有Logistic 和 Tanh函数。</p>
<p><strong>Logistic 函数</strong> Logistic函数定义为：<br>$$<br>\sigma（x) = \frac{1}{1+exp(-x)}<br>$$<br>     Logistic函数可以看成一个“挤压”函数，把实数域输入压入到（0,1）。当输入值在0 附近时，Sigmoid 型函数近似为线性函数；当输入值靠近两端时，对输入进行抑制。输入越小，越接近于0；输入越大，越接近于1. 类似生物神经元，对一些输入会产生兴奋（输出为1），对另一些输入产生抑制（输出为0）. Logistic 函数是连续可导的。包含Logistic函数的神经元具有以下性质：</p>
<ul>
<li>其输出直接可以看作是概率分布，使得神经网络可以更好地和统计学习模型进行结合。</li>
<li>其可以看作是一个软性门（Soft Gate），用来控制其他神经元输出信息的数量。</li>
</ul>
<p><strong>Tanh 函数</strong> Tanh函数定义为：<br>$$<br>tanh(x) = \frac{exp(x) - exp(-x)}{exp(x) + exp(-x)}<br>$$<br>​    Tanh函数可以看做是放大并平移的Logistic函数，值域为（-1,1）：<br>$$<br>tanh(x) = 2\sigma(2x) - 1<br>$$<br>​    下图是Logistic函数和Tanh函数的形状。Tanh 函数的输出是零中心化的（Zero-Centered），而Logistic函数的输出恒大于0。非零中心化的输出会使得其后一层的神经元的输入发生偏置偏移（Bias Shift），并进一步使得梯度下降的收敛速度变慢。</p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C2.jpg" alt="2"></p>
<h4 id="1-1-1-Hard-Logistic-函数和-Hard-Tanh-函数"><a href="#1-1-1-Hard-Logistic-函数和-Hard-Tanh-函数" class="headerlink" title="1.1.1 Hard-Logistic 函数和 Hard-Tanh 函数"></a>1.1.1 Hard-Logistic 函数和 Hard-Tanh 函数</h4><p>​    Logistic函数和Tanh函数在中间（0附近）近似线性，在两端饱和，计算开销较大，可用分段函数近似。</p>
<p>​    Logistic函数$\sigma(x)$ 的导数为 $\sigma^{‘} (x) = \sigma(x)(1-\sigma(x))$ . Logistic函数在0附近的一阶泰勒展开为：<br>$$<br>\begin{align}<br>g_l(x) &amp; \approx \sigma(0) + x \times \sigma^{‘} (0)  \<br> &amp; =0.25x + 0.5 \<br> \end{align}<br>$$<br>从而得到Logistic函数的近似函数hard-logistic：<br>$$<br>\begin{align}<br>hard-logistic(x) &amp; =\begin{cases} 1, &amp; \text {$g_l \geq 1$} \ g_l, &amp; \text{$ 0 &lt; g_l &lt; 1$} \ 0, &amp; \text{$g_l \leq 1$} \end{cases}  \<br> &amp; = max(min(g_l(x), 1), 0) \<br> &amp; = max(min(0.25x+0.5, 1), 0) \<br> \end{align}<br>$$<br>​    同理，Tanh函数在0附近的一阶泰勒展开为：<br>$$<br>\begin{align}<br>g_t(x) &amp; \approx tanh(0) + x \times tanh^{‘} (0)  \<br> &amp; = x \<br> \end{align}<br>$$<br>从而得到Tanh函数的近似函数hard-tanh：<br>$$<br>\begin{align}<br>hard-tanh(x) &amp; = max(min(g_t(x), 1),  -1)  \<br> &amp; = max(min(x, 1),  -1) \<br> \end{align}<br>$$<br>​    Hard-Logistic 函数和Hard-Tanh 函数的图像如下：</p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C3.jpg" alt="3"></p>
<h3 id="1-2-ReLU函数"><a href="#1-2-ReLU函数" class="headerlink" title="1.2 ReLU函数"></a>1.2 ReLU函数</h3><p>​    ReLU（Rectified Linear Unit，修正线性单元）是常用的激活函数：<br>$$<br>\begin{align}<br>ReLU(x) &amp; =\begin{cases} x, &amp; \text {$x \geq 0$} \ 0, &amp; \text{$ x &lt; 0$}\end{cases}  \<br> &amp; = max(0, x) \<br> \end{align}<br>$$<br><strong>优点</strong> </p>
<ul>
<li>计算高效：只有加、乘、比较</li>
<li>生物可解释性强：单侧抑制、宽兴奋边界</li>
<li>稀疏性好：约50%的神经元处于激活状态</li>
<li>优化性好：缓解梯度消失、加速梯度下降的收敛速度</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>输出非零中心化，给最后一层神经网络带来<strong>偏置偏移</strong>，影响梯度下降的效率</li>
<li>ReLU神经元可能在训练数据上都不能被激活，即其梯度更新后永远为0</li>
</ul>
<h4 id="1-2-1-带泄露的ReLU"><a href="#1-2-1-带泄露的ReLU" class="headerlink" title="1.2.1 带泄露的ReLU"></a>1.2.1 带泄露的ReLU</h4><p>​    带泄露的ReLU（Leaky ReLU）在输入 x&lt;0 时，保持一个很小的 $\lambda$ .当神经元没有激活时，也可以有一个非零的梯度可以参与参数更新。<br>$$<br>\begin{align}<br>LeakyReLU(x) &amp; =\begin{cases} x, &amp; \text {if x &gt; 0} \ \lambda x, &amp; \text{if x $\leq$ 0}\end{cases}  \<br> &amp; = max(0, x) + \lambda min(0, x) \<br> \end{align}<br>$$<br>$\lambda$ 是一个很小的常数如0.01. 当$\lambda$ &lt; 1 时带泄露的LeakyReLU可以看做一个简单的maxout单元：<br>$$<br>LeakyReLU(x) = max(x, \lambda x)<br>$$</p>
<h4 id="1-2-2-带参数的ReLU"><a href="#1-2-2-带参数的ReLU" class="headerlink" title="1.2.2 带参数的ReLU"></a>1.2.2 带参数的ReLU</h4><p>​    带参数的ReLU（Parametric ReLU，PReLU）引入一个可学习的参数。不同的神经元可有不同的参数。一组神经元可共享一个参数。对于第i的神经元的PReLU:<br>$$<br>\begin{align}<br>PReLU_i (x) &amp; =\begin{cases} x, &amp; \text {if x &gt; 0} \ \gamma_i x, &amp; \text{if x $\leq$ 0}\end{cases}  \<br> &amp; = max(0, x) + \gamma_i min(0, x) \<br> \end{align}<br>$$<br>如果 $\gamma_i = 0$ 那么PReLU退化为ReLU。如果 $\gamma_i$ 是一个非常小的常数，PReLU可看做LeakyReLU。</p>
<h4 id="1-2-3-ELU"><a href="#1-2-3-ELU" class="headerlink" title="1.2.3 ELU"></a>1.2.3 ELU</h4><p>​    ELU（Exponential Linear Unit，指数线性单元）是一个近似的零中心化的非线性函数。一种$\lambda \geq 0$ 是超参数，将输出均值调整在0附近：<br>$$<br>\begin{align}<br>ELU (x) &amp; =\begin{cases} x, &amp; \text {if x &gt; 0} \ \gamma(exp(x)-1), &amp; \text{if x $\leq$ 0}\end{cases}  \<br> &amp; = max(0, x) +  min(0, \gamma(exp(x) - 1)) \<br> \end{align}<br>$$</p>
<h4 id="1-2-4-Softplus函数"><a href="#1-2-4-Softplus函数" class="headerlink" title="1.2.4 Softplus函数"></a>1.2.4 Softplus函数</h4><p>​    Softplus函数可以看作是ReLU的平滑版本：<br>$$<br>softplus(x) = log(1+exp(x))<br>$$<br>Softplus函数导数是Logistic函数。有单侧抑制、宽兴奋边界的特性，无稀疏激活性。</p>
<p>几种ReLU激活函数图像：</p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C4.jpg" alt="4"></p>
<h3 id="1-3-Swish函数"><a href="#1-3-Swish函数" class="headerlink" title="1.3 Swish函数"></a>1.3 Swish函数</h3><p>​    Swish 函数是一种自门控（Self-Gated）激活函数，定义为：<br>$$<br>swish(x) = x\sigma(\beta x)<br>$$<br>$\sigma(·)$ 为Logistic函数，$\beta$ 是可学习的参数或固定参数。$\sigma(·) \in (0, 1)$ 可以看做软性的门控制机制。当$\sigma(\beta x)$ 接近于1时，门开状态，当$\sigma(\beta x)$ 接近于0时，门关状态。</p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C5.jpg" alt="5"></p>
<h3 id="1-4-高斯误差线性单元"><a href="#1-4-高斯误差线性单元" class="headerlink" title="1.4 高斯误差线性单元"></a>1.4 高斯误差线性单元</h3><p>​    高斯误差线性单元（Gaussian Error Linear Unit，GELU）和Swish 函数比较类似：<br>$$<br>GELU(X) = xP(X \leq x)<br>$$<br>$P(X \leq x)$ 是高斯分布，常取均值为0，方差为1的正太高斯分布。高斯分布为S型函数，故GELU可用Tanh或Logistic近似：<br>$$<br>GELU(X) \approx 0.5x(1 + tanh(\sqrt{\frac{2}{\pi}} (x + 0.044715x^3))) \<br>GELU(X) \approx x \sigma(1.702x)<br>$$</p>
<h3 id="1-5-Maxout单元"><a href="#1-5-Maxout单元" class="headerlink" title="1.5 Maxout单元"></a>1.5 Maxout单元</h3><p>​    Sigmoid型激活函数输入是一个神经元的净输入z， 是一个标量。Maxout单元的输入是上一层神经元的全部原始输出，是一个向量 $\pmb{𝒙} = [𝑥<em>1 ; 𝑥_2; \cdots ; 𝑥_𝐷]$ 。每个Maxout单元有K个权重向量$\pmb{w}_k \in \mathbb{R}^D$ 和偏置 $b_k (1\leq k \leq K)$ 。对于输入 <strong>x</strong> 可以得到K个净输入 $z_k (1\leq k \leq K)$：<br>$$<br>z_k = \pmb{w}_k^T \pmb{x} + b_k<br>$$<br>其中 $\pmb{w}_k = {[w</em>{k,1} ; \cdots ; w_{k,D} ]}^T$ 为第k个权重向量。</p>
<p>​    Maxout单元的非线性函数定义：<br>$$<br>maxout(\pmb{x}) = \mathop{\max}_{k \in [1, K]} (z_k)<br>$$<br>​    Maxout可以看做是整体学习输入到输出之间的非线性映关系。</p>
<h2 id="2-前馈神经网络"><a href="#2-前馈神经网络" class="headerlink" title="2. 前馈神经网络"></a>2. 前馈神经网络</h2><p>​    给定一组神经元，我们可以以神经元为节点来构建一个网络。在前馈神经网络中，各神经元分别属于不同的层。每一层的神经元可以接收前一层神经元的信号，并产生信号输出到下一层。 第0 层称为<strong>输入层</strong>，最后一层称为<strong>输出层</strong>，其他中间层称为<strong>隐藏层</strong>. 整个网络中无反馈，信号从输入层向输出层单向传播，可用一个有向无环图表示。</p>
<p>​    前馈网络可以看作一个函数，通过简单非线性函数的多次复合，实现输入空间到输出空间的复杂映射。这种网络结构简单，易于实现。结构示意图如下：</p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C6.jpg" alt="6"></p>
<p>​    前馈神经网络的符号表示：</p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C7.jpg" alt="7"></p>
<p>​    令$\pmb{a}^{(0)} = \pmb{x}$ ，前馈神经网络通过不断迭代下面公式进行信息传播：<br>$$<br>\pmb{z}^{(l)} = \pmb{W}^{(l)} \pmb{a}^{(l-1)} +\pmb{b}^{(l)} \<br>\pmb{a}^{(l)} = f_l (\pmb{z}^{(l)})<br>$$<br>首先根据第 $l-1$ 层神经元的活性值 $ \pmb{a}^{(l-1)}$ 计算出第 $l$ 层神经元的净活性值 $ \pmb{z}^{(l)}$，然后经过一个激活函数得到第 $l$ 层神经元的活性值。即：<br>$$<br>\pmb{z}^{(l)} = \pmb{W}^{(l)} f_{l-1}(\pmb{z}^{(l-1)}) +\pmb{b}^{(l)} \<br>\pmb{a}^{(l)} = f_l ( \pmb{W}^{(l)} \pmb{a}^{(l-1)} +\pmb{b}^{(l)} )<br>$$<br>​    这样，前馈神经网络就可以逐层传递信息，得到最终输出 $\pmb{a}^{(L)}$ 。整个网络可看做一个复合函数：$\phi(\pmb{x}; \pmb{W},  \pmb{b})$ ,将向量<strong>x</strong>作为第1层的输入 $\pmb{a}^{(0)}$ ，将第 $L$ 层的输出$\pmb{a}^{(L)}$ 作为整个函数的输出：<br>$$<br>\pmb{x} = \pmb{a}^{(0)} \rightarrow \pmb{z}^{(1)} \rightarrow  \pmb{a}^{(1)} \rightarrow \pmb{z}^{(2)} \rightarrow \cdots \rightarrow   \pmb{a}^{(L-1)} \rightarrow \pmb{z}^{(L)} \rightarrow  \pmb{a}^{(L)} = \phi(\pmb{x}; \pmb{W},  \pmb{b})<br>$$<br>其中W, b表示网络中所有层的连接权重和偏置。</p>
<h3 id="2-1-通用近似定理"><a href="#2-1-通用近似定理" class="headerlink" title="2.1 通用近似定理"></a>2.1 通用近似定理</h3><p>​    前馈神经网络具有很强的拟合能力，常见的连续非线性函数都可以用前馈神经网络来近似。可用通用近似定理解释：</p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C8.jpg" alt="8"></p>
<p>​    实际中，真实的映射函数并不知道，一般通过经验风险最小化和正则化来进行参数学习。</p>
<h3 id="2-2-应用到机器学习"><a href="#2-2-应用到机器学习" class="headerlink" title="2.2 应用到机器学习"></a>2.2 应用到机器学习</h3><p>​    机器学习中，输入样本的特征对分类器的影响很大。要取得好的分类效果，需要将样本的原始特征向量 <strong>𝒙</strong> 转换到更有效的特征向量 $ 𝜙(𝒙) \in \mathbb{R}^{D^{‘}} $，这个过程叫做特征抽取。</p>
<p>​    前多层前馈神经网络可以看作是一个非线性复合函数 $\phi : \mathbb{R}^D  \rightarrow  \mathbb{R}^{D^{‘}} $，将输入 $ 𝒙 \in \mathbb{R}^D $ 映射到输出 $ 𝒙 \in \mathbb{R}^{D^{‘}} $ 。 因此，多层前馈神经网络也可以看成是一种特征转换方法，其输出𝜙(𝒙) 作为分类器的输入进行分类。</p>
<p>​    给定一个样本（x, y) ，先利用多层前馈神经网络将𝒙 映射到𝜙(𝒙)，然后再将𝜙(𝒙) 输入到分类器𝑔(⋅)，得到分类器的输出 $\hat{y}$ ：<br>$$<br>\hat{y} = g(\phi(\pmb{𝒙}) ;\theta )<br>$$<br>​    对于二分类问题 𝑦 ∈ {0, 1}，采用Logistic回归分类器，神经网络最后一层只有一个神经元，激活函数为Logistic函数，网络输出可直接作为类别 y = 1 的后验概率：<br>$$<br>p(y=1|\pmb{x}) = a^{(L)}<br>$$<br>​    对于多分类问题 𝑦 ∈ {1, ⋯ , 𝐶}，使用Softmax 回归分类器，神经网络最后一层设置𝐶 个神经元，激活函数为Softmax 函数.。网络最后一层（第𝐿 层）的输出可以作为每个类的后验概率：<br>$$<br>\hat{\pmb{y}} = softmax(\pmb{z}^{(L)})<br>$$<br>$\hat{y} \in \mathbb{R}^C$ 第 𝐿 层神经元的活性值，每一维分别表示不同类别标签的预测后验概率。</p>
<h3 id="2-3-参数学习"><a href="#2-3-参数学习" class="headerlink" title="2.3 参数学习"></a>2.3 参数学习</h3><p>​    采用交叉熵损失函数，对于样本（x, y) ，损失函数为：<br>$$<br>L(\pmb{y}, \hat{\pmb{y}}) = -\pmb{y}^T \log \hat{\pmb{y}}<br>$$<br>其中𝑦 ∈ {0, 1} 为标签y对应的one-hat向量表示。</p>
<p>​    给定训练集$D = {(\pmb{x}^{(n)},  y^{(n)})}<em>{n=1}^N$ , 其在数据集D上的结构化风险函数为：<br>$$<br>R(\pmb{W}, \pmb{b}) = \frac{1}{N} \sum</em>{n=1}^{N}{L(\pmb{y}^{(n)}, \hat{\pmb{y}}^{(n)})} + \frac{1}{2} \lambda \parallel \pmb{W} \parallel <em>{F}^{2}<br>$$<br>其中 <strong>W</strong> 和 <strong>b</strong> 是权重矩阵和偏置向量；$\parallel \pmb{W} \parallel _{F}^{2}$ 是正则化项，用来放置过拟合；$\lambda &gt; 0$ 是超参数。$\lambda$ 越大， <strong>W</strong> 越接近于0。这里的 $\parallel \pmb{W} \parallel _{F}^{2}$ 一般使用Frobenius 范数：<br>$$<br>\parallel \pmb{W} \parallel _{F}^{2} =  \sum</em>{l=1}^{L} \sum_{i=1}^{M_l} \sum_{j=1}^{M_{l-1}} {(w_{ij}^{(l)})}^2<br>$$<br>利用梯度下降法进行学习更新参数，第l层的参数更新方式为：</p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C9.jpg" alt="9"></p>
<h2 id="3-反向传播算法"><a href="#3-反向传播算法" class="headerlink" title="3. 反向传播算法"></a>3. 反向传播算法</h2><p>​    采用随机梯度下降进行神经网络参数学习。需要计算损失函数对参数的偏导数，如果通过链式法则逐一对每个参数进行求偏导比较低效。在神经网络的训练中经常使用反向传播算法来高效地计算梯度。</p>
<p>​    对第𝑙 层中的参数 $\pmb{W}^{(l)}$ 和 $\pmb{b}^{(l)}$ 算偏导数. 因为$\frac{\partial L(y, \hat{y})}{\partial W^{(l)}}$的计算涉及向量对矩阵的微分，十分繁琐，故先计算 L(𝒚, 𝒚̂) 关于参数矩阵中每个元素的偏导数$\frac{\partial L(y, \hat{y})}{\partial W^{(l)}_{i,j}}$根据链式法则:</p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C10.jpg" alt="10"></p>
<p>上述公式第二项都是目标函数关于第𝑙 层的神经元𝒛(𝑙)的偏导数，称为误差项，可以一次计算得到。故只计算3个偏导数：</p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C11.jpg" alt="11"></p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C12.jpg" alt="12"></p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C13.jpg" alt="13"></p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C14.jpg" alt="14"></p>
<p>​    反向传播算法的含义是：第 𝑙 层的一个神经元的误差项是所有与该神经元相连的第𝑙 + 1 层的神经元的误差项的权重和。然后，再乘上该神经元激活函数的梯度。</p>
<p>​    由以上公式可得到：$\frac{\partial L(y, \hat{y})}{\partial W^{(l)}_{i,j}}$ </p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C15.jpg" alt="15"></p>
<p>​    从而得到 L(𝒚, 𝒚̂) 对第𝑙 层中的参数 $\pmb{W}^{(l)}$ 和 $\pmb{b}^{(l)}$ 的梯度 $\frac{\partial L(y, \hat{y})}{\partial W^{(l)}}$ 和 $\frac{\partial L(y, \hat{y})}{\partial  b^{(l)}}$ ：</p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C16.jpg" alt="16"></p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C17.jpg" alt="17"></p>
<p>​    反向传播算法：</p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C18.jpg" alt="18"></p>
<h2 id="4-梯度自动计算"><a href="#4-梯度自动计算" class="headerlink" title="4. 梯度自动计算"></a>4. 梯度自动计算</h2><p>​    手动用链式法则求导并转换为计算机程序的过程非常琐碎并容易出错，导致实现神经网络变得十分低效. 实际上，参数的梯度可以让计算机来自动计算。自动计算梯度的方法有：数值微分、符号微分和自动微分。</p>
<h3 id="4-1-数值微分"><a href="#4-1-数值微分" class="headerlink" title="4.1 数值微分"></a>4.1 数值微分</h3><p>​    利用数值方法计算导数，导数 f(x) 在x处的导数定义：</p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C19.jpg" alt="19"></p>
<p>用定义法求导，由于Δ𝑥的原因容易带来舍入误差和截断误差，故实际中常采用一下凡是计算梯度，减少误差：</p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C20.jpg" alt="20"></p>
<p>假设每次正向传播的计算复杂度为𝑂(𝑁)，则计算数值微分的总体时间复杂度为𝑂($N^2$).</p>
<h3 id="4-2-符号微分"><a href="#4-2-符号微分" class="headerlink" title="4.2 符号微分"></a>4.2 符号微分</h3><p>​    符号计算也叫代数计算，是指用计算机来处理带有变量的数学表达式. 这里的变量看作是符号（Symbols），一般不需要代入具体的值. 符号计算的输入和输出都是数学表达式，一般包括对数学表达式的化简、因式分解、微分、积分、解代数方程、求解常微分方程等运算。</p>
<h3 id="4-3-自动微分"><a href="#4-3-自动微分" class="headerlink" title="4.3 自动微分"></a>4.3 自动微分</h3><p>​    自动微分（Automatic Differentiation，AD）是一种可以对一个（程序）函数进行计算导数的方法。符号微分的处理对象是数学表达式，而自动微分的处理对象是一个函数或一段程序。自动微分可以直接在原始程序代码进行微分，因此自动微分成为目前大多数深度学习框架的首选。</p>
<p>​    对 $f(x; w; b) = \frac{1}{exp(-(wx + b)) + 1}$ 进行自动微分举例:</p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C21.jpg" alt="21"></p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C22.jpg" alt="22"></p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C23.jpg" alt="23"></p>
<p><strong>符号微分和自动微分</strong> 符号微分和自动微分都利用计算图和链式法则来自动求解导数。符号微分在编译阶段先构造一个复合函数的计算图，通过符号计算得到导数的表达式，还可以对导数表达式进行优化，在程序运行阶段才代入变量的具体数值来计算导数。而自动微分则无需事先编译，在程序运行阶段边计算边记录计算图，计算图上的局部梯度都直接代入数值进行计算，然后用前向或反向模式来计算最终的梯度。</p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C24.jpg" alt="24"></p>
<p><strong>静态计算图和动态计算图</strong> 计算图按构建方式可以分为静态计算图和动态计算图。静态计算图是在编译时构建计算图，计算图构建好之后在程序运行时不能改变，而动态计算图是在程序运行时动态构建。静态计算图在构建时可以进行优化，并行能力强，但灵活性比较差。动态计算图则不容易优化，当不同输入的网络结构不一致时，难以并行计算，但是灵活性比较高。</p>
<h2 id="5-优化问题"><a href="#5-优化问题" class="headerlink" title="5. 优化问题"></a>5. 优化问题</h2><h3 id="5-1-非凸优化问题"><a href="#5-1-非凸优化问题" class="headerlink" title="5.1 非凸优化问题"></a>5.1 非凸优化问题</h3><p>​    神经网络不同于线性模型，它的优化问题常是非凸优化问题。对于最简单的1-1-1结构的两层神经网络问题：</p>
<p><img src=".%5C%E3%80%8A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%5C25.jpg" alt="25"></p>
<h3 id="5-2-梯度消失问题"><a href="#5-2-梯度消失问题" class="headerlink" title="5.2 梯度消失问题"></a>5.2 梯度消失问题</h3><p>​    由于Sigmoid 型函数的饱和性，饱和区的导数更是接近于0。这样，误差经过每一层传递都会不断衰减。当网络层数很深时，梯度就会不停衰减，甚至消失，使得整个网络很难训练。这就是所谓的梯度消失问题（Vanishing Gradient Problem），也称为梯度弥散问题。</p>
<p>​    减轻梯度消失问题的方法有很多种。一种简单有效的方式是使用导数比较大的激活函数，比如ReLU 等。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/06/05/%E5%87%BA%E7%94%9F/" rel="next" title="出生">
                <i class="fa fa-chevron-left"></i> 出生
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">zys</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%20%7C%7C%20archive">
              
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-神经元和激活函数"><span class="nav-number">1.</span> <span class="nav-text">1. 神经元和激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-Sigmoid-型函数"><span class="nav-number">1.1.</span> <span class="nav-text">1.1  Sigmoid 型函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-1-Hard-Logistic-函数和-Hard-Tanh-函数"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1.1 Hard-Logistic 函数和 Hard-Tanh 函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-ReLU函数"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 ReLU函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-1-带泄露的ReLU"><span class="nav-number">1.2.1.</span> <span class="nav-text">1.2.1 带泄露的ReLU</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-2-带参数的ReLU"><span class="nav-number">1.2.2.</span> <span class="nav-text">1.2.2 带参数的ReLU</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-3-ELU"><span class="nav-number">1.2.3.</span> <span class="nav-text">1.2.3 ELU</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-4-Softplus函数"><span class="nav-number">1.2.4.</span> <span class="nav-text">1.2.4 Softplus函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-Swish函数"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 Swish函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-高斯误差线性单元"><span class="nav-number">1.4.</span> <span class="nav-text">1.4 高斯误差线性单元</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-Maxout单元"><span class="nav-number">1.5.</span> <span class="nav-text">1.5 Maxout单元</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-前馈神经网络"><span class="nav-number">2.</span> <span class="nav-text">2. 前馈神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-通用近似定理"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 通用近似定理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-应用到机器学习"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 应用到机器学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-参数学习"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 参数学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-反向传播算法"><span class="nav-number">3.</span> <span class="nav-text">3. 反向传播算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-梯度自动计算"><span class="nav-number">4.</span> <span class="nav-text">4. 梯度自动计算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-数值微分"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 数值微分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-符号微分"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 符号微分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-自动微分"><span class="nav-number">4.3.</span> <span class="nav-text">4.3 自动微分</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-优化问题"><span class="nav-number">5.</span> <span class="nav-text">5. 优化问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-非凸优化问题"><span class="nav-number">5.1.</span> <span class="nav-text">5.1 非凸优化问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-梯度消失问题"><span class="nav-number">5.2.</span> <span class="nav-text">5.2 梯度消失问题</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">zys</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
